---
title: "Visualizing Text and Distributions"
output: 
  html_document:
    keep_md: true
    toc: true
    toc_float: true
---

# Data Visualization Project 03


In this exercise you will explore methods to visualize text data and practice how to recreate charts that show the distributions of a continuous variable. 


## Part 1: Density Plots

Using the dataset obtained from FSU's [Florida Climate Center](https://climatecenter.fsu.edu/climate-data-access-tools/downloadable-data), for a station at Tampa International Airport (TPA) from 2016 to 2017, attempt to recreate the charts shown below

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
weather_tpa <- read_csv("https://github.com/reisanar/datasets/raw/master/tpa_weather_16_17.csv")
# random sample 
sample_n(weather_tpa, 4)
```

See https://www.reisanar.com/slides/relationships-models#10 for a reminder on how to use this dataset with the `lubridate` package for dates and times.


(a) Recreate the plot below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_facet.png")
```

Hint: the option `binwidth = 3` was used with the `geom_histogram()` function.

```{r}
str(weather_tpa)
```

```{r}
library(lubridate)
weather_tpa01 <- weather_tpa %>% 
  mutate(month_name = month(month, label = TRUE, abbr = FALSE))

str(weather_tpa01)
```

```{r}

P3.1 <- ggplot(weather_tpa01, aes(x = max_temp, fill = month_name)) +
          geom_histogram(binwidth = 3, color = "white", boundary = 1) +
          scale_x_continuous(breaks = seq(0, 12, by = 1)) + 
          guides(fill = FALSE) +
          facet_wrap(vars(month_name)) +
          labs(x = "Maximun temperature", y= "Number of Days") 
P3.1
```

```{r}
library("svglite")
ggsave("P3.1.jpg", P3.1)
```


(b) Recreate the plot below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_density.png")
```

Hint: check the `kernel` parameter of the `geom_density()` function, and use `bw = 0.5`.

```{r}
P3.2 <- ggplot(weather_tpa01, aes(x = max_temp)) +
          geom_density(color = "grey20", fill = "grey50",
               bw = 0.5, kernel = "epanechnikov")+
          labs(x = "Maximun temperature") 

P3.2
```
```{r}
ggsave("P3.2.jpg", P3.2)
```



(c) Recreate the chart below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_density_facet.png")
```

Hint: default options for `geom_density()` were used. 

```{r}
P3.3 <- ggplot(weather_tpa01, aes(x = max_temp, fill = month_name)) +
          geom_density(binwidth = 3, color = "black", boundary = 1) +
          scale_x_continuous(breaks = seq(0, 12, by = 1)) + 
          guides(fill = FALSE) +
          facet_wrap(vars(month_name)) +
          labs(x = "Maximun temperature", y= "Number of Days") 

P3.3
```

```{r}
ggsave("P3.3.jpg", P3.3)
```


(d) Recreate the chart below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_ridges.png")
```

Hint: default options for `geom_density()` were used. 

```{r}
library("ggridges")
P3.4 <- ggplot(weather_tpa01, aes(x = max_temp, y = month_name, fill = month_name)) +
          geom_density_ridges() +
          guides(fill = FALSE)+
          labs(x = "Maximun temperature", y= "") 

P3.4
```

```{r}
ggsave("P3.4.jpg", P3.4)
```


(e) Recreate the plot below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_ridges.png")
```

Hint: use the`ggridges` package, and the `geom_density_ridges()` function paying close attention to the `quantile_lines` and `quantiles` parameters.

```{r}
P3.5 <- ggplot(weather_tpa01, aes(x = max_temp, y = month_name, fill = month_name)) +
          geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +
          guides(fill = FALSE)+
          labs(x = "Maximun temperature", y= "") 

P3.5
```

```{r}
ggsave("P3.5.jpg", P3.5)
```


(f) Recreate the chart below:

```{r, echo = FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("https://github.com/reisanar/figs/raw/master/tpa_max_temps_ridges_plasma.png")
```

Hint: this uses the `plasma` option (color scale) for the _viridis_ palette.

```{r}
P3.6 <- ggplot(weather_tpa01, aes(x = max_temp, y = month_name, fill = ..x..)) +
         geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +
         labs(x = "Maximun temperature(in Fahrenheit degrees)", y= NULL, fill = "")+
         scale_fill_viridis_c(option = "plasma")

P3.6  
```

```{r}
ggsave("P3.6.jpg", P3.6)
```


## Part 2: Visualizing Text Data

Review the set of slides (and additional resources linked in it) for visualizing text data: https://www.reisanar.com/slides/text-viz#1

Choose any dataset with text data, and create at least one visualization with it. For example, you can create a frequency count of most used bigrams, a sentiment analysis of the text data, a network visualization of terms commonly used together, and/or a visualization of a topic modeling approach to the problem of identifying words/documents associated to different topics in the text data you decide to use. 

Make sure to include a copy of the dataset in the `data/` folder, and reference your sources if different from the ones listed below:

- [Billboard Top 100 Lyrics](https://github.com/reisanar/datasets/blob/master/BB_top100_2015.csv)

- [RateMyProfessors comments](https://github.com/reisanar/datasets/blob/master/rmp_wit_comments.csv)

- [FL Poly News 2020](https://github.com/reisanar/datasets/blob/master/poly_news_FL20.csv)

- [FL Poly News 2019](https://github.com/reisanar/datasets/blob/master/poly_news_FL19.csv)

(to get the "raw" data from any of the links listed above, simply click on the `raw` button of the GitHub page and copy the URL to be able to read it in your computer using the `read_csv()` function)

```{r}
B100_lyrics15 <- read_csv("https://raw.githubusercontent.com/reisanar/datasets/master/BB_top100_2015.csv")
# random sample 
sample_n(B100_lyrics15, 5)
```

```{r}
# Tokenization
library(tidytext)
library(tidyverse)

B100_lyrics15 %>% 
  filter(Rank %in% 1:10) %>% 
  unnest_tokens(word, Lyrics)
```

```{r}
# Remove stop words
B100_lyrics15 %>%
  filter(Rank %in% 1:10) %>%
  unnest_tokens(word, Lyrics, token = "words") %>%
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
```

```{r}
# Top 15 songs lyrics in 2015 (no stop-words)
B100_lyrics15_top15 <- B100_lyrics15 %>%
  filter(Rank %in% 1:15) %>%
  unnest_tokens(word, 
                Lyrics, 
                token = "words") %>%
  filter(!word %in% stop_words$word, 
         str_detect(word, "[a-z]"))
```

```{r}
# check some of the most frequent words
B100_lyrics15_top15 %>% 
  group_by(word) %>% 
  summarise(uses = n()) %>% 
  arrange(desc(uses)) %>% 
  head(15)
```
### Words used most frequently

```{r}
library(viridis)
```

```{r}
P3.7 <- B100_lyrics15_top15 %>% 
        group_by(word) %>% 
        summarise(uses = n()) %>% 
        arrange(desc(uses)) %>% 
        slice(1:15) %>% 
          ggplot() + 
            geom_bar(aes(x = word, y = uses, fill = factor(word), colour = "black"), stat = "identity") + 
            labs(x = "", y = "", title = "The most frequent word", subtitle = "Top 15 Billboard songs in 2015")+
            coord_polar() + 
            theme_minimal()+
               theme(
                     legend.position = "none",
                     axis.title.x = element_blank(),
                     axis.title.y = element_blank(),
                     axis.ticks = element_blank(),
                     axis.text.y = element_blank(),
                     axis.text.x = element_text(face = "bold"),
                     plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
                     plot.subtitle = element_text(size = 12, hjust = 0.5))+
               scale_fill_viridis_d(option = "plasma")

P3.7
```

```{r}
ggsave("The most frequent word.jpg", P3.7)
```


Much as removing stop words is an anti-join operation, performing sentiment analysis is an inner join operation.

```{r}
B100_lyrics15_top15 %>%
  inner_join(get_sentiments("bing")) %>%
  count(Song, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r}
P3.8 <- B100_lyrics15_top15 %>%
        inner_join(get_sentiments("bing")) %>%
        count(Song, sentiment) %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative) %>% 
           ggplot() + 
           geom_bar(aes(x = reorder(Song, sentiment), y = sentiment, fill = sentiment), stat = "identity") + 
           coord_flip() + 
          labs(x = "", y = "Sentiment", title = "Sentiment Analysis of Songs using bing lexicon", 
            subtitle = "Top 15 Billboard songs in 2015") + 
          theme_minimal()+
          theme(legend.position = "none",
                     plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
                  plot.subtitle = element_text(size = 12, hjust = 0.5)) +
         scale_fill_viridis(option = "mako") 
P3.8 
 
```

```{r}
ggsave("Sentiment Analysis of Songs using bing lexicon.jpg", P3.8)
```